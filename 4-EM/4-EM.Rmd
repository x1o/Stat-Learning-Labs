---
title: "4-EM"
author: "Dmitry Zotikov"
date: "April 16, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Plot of the data

```{r}
library('mvtnorm')

plot.mixture <- function(model, YY, ...) {
  plot(YY, ...)
  mmu <- model$mmu
  SS <- model$SS
  m <- length(mmu)
  xx <- seq(min(YY[,1]), max(YY[,1]), by=.1)
  yy <- seq(min(YY[,2]), max(YY[,2]), by=.1)
  for (ell in 1:m) {
    contour(xx, yy, 
             outer(xx, yy, function(xx, yy) dmvnorm(cbind(xx, yy), mmu[[ell]], SS[[ell]])),
             add=T, col=1+ell%%7)
  }
}

YY <- as.matrix(faithful)
par(mfrow=c(1,2))
# boxplot(YY$eruptions)
# boxplot(YY$waiting)
par(mfrow=c(1,1))
plot(YY)
```

# EM & LLK

```{r}
llk <- function(model, YY) {
  mmu <- model$mmu
  SS <- model$SS
  pp <- model$pp
  m <- length(mmu)
  N <- nrow(YY)
  PPHI <- matrix(0, N, m)
  for (ell in 1:m) {
    PPHI[,ell] <- dmvnorm(YY, mean=mmu[[ell]], sigma=SS[[ell]])
  }
  sum(log(PPHI %*% pp))
}

em.gauss <- function(m, YY, llk.diff.eps=0.1, do.plot=FALSE, verbose=FALSE) {
  d <- ncol(YY)
  N <- nrow(YY)
  phi <- dmvnorm
  mmu <- list()
  SS <- list()
  ww <- list()
  zz <- list()
  llk.prev <- -Inf
  llk.hist <- NULL
  # Initialization
  # TODO: Split the data into m clusters at random; calculate mmu, SS, pp for each cluster
  for (ell in 1:m) {
    # pick two data points at random
    mmu[[ell]] <- sample(as.data.frame(t(YY)), 1)[[1]]
    SS[[ell]] <- cov(YY)
  }
  pp <- rep(1/m, m)
  if (do.plot) {
    plot.mixture(list(pp=pp, mmu=mmu, SS=SS), YY, main=paste('n.iter =', 0))
  }
  n.iter <- 1
  repeat {
    # Expectation
    PPHI <- matrix(0, N, m)
    for (ell in 1:m) {
      PPHI[,ell] <- phi(YY, mean=mmu[[ell]], sigma=SS[[ell]])
    }
    for (ell in 1:m) {
      zz[[ell]] <- pp[[ell]] * PPHI[,ell]  / PPHI %*% pp
    }
    for (ell in 1:m) {
      ww[[ell]] <- zz[[ell]] / sum(zz[[ell]])
    }
    # Maximization
    for (ell in 1:m) {
      mmu[[ell]] <- as.vector(t(YY) %*% ww[[ell]])
      pp[[ell]] <- sum(zz[[ell]]) / N
      # FIXME: vectorize
      for (i1 in 1:d) {
        for (i2 in 1:d) {
          SS[[ell]][i1, i2] <- sum(ww[[ell]] * (YY[, i1] - mmu[[ell]][i1]) * (YY[, i2] - mmu[[ell]][i2]))
        }
      }
    }
    llk.cur <- llk(list(pp=pp, mmu=mmu, SS=SS), YY)
    llk.diff <- abs(llk.cur - llk.prev)
    if (verbose) {
      cat('# ', n.iter, ' LLK = ', llk.cur, ' (', llk.diff, ')\n', sep='')
    }
    llk.hist <- c(llk.hist, llk.cur)
    if (do.plot) {
      plot.mixture(list(pp=pp, mmu=mmu, SS=SS), YY, main=paste('n.iter =', n.iter))
    }
    if (llk.diff < llk.diff.eps) {
      if (do.plot) {
        plot(llk.hist, type='o', col='blue', main='Log likelihood', xlab='n.iter')
      }
      return(list(pp=pp, mmu=mmu, SS=SS, zz=zz, llk=llk.cur, llk.hist=llk.hist))
    } else {
      llk.prev <- llk.cur
      n.iter <- n.iter + 1
    }
  }
}
```

## EM for $m = 2$

```{r}
set.seed(2)
model.2 <- em.gauss(2, YY, do.plot=TRUE, verbose=TRUE)
model.2$pp
model.2$mmu
model.2$SS
```

## EM for $m = 3$

```{r}
set.seed(1)
model.3 <- em.gauss(3, YY, do.plot=TRUE, verbose=TRUE)
```

## Fit for several initial parameters

Choose the best model out of several runs with the highest LLK.

```{r}
fit.em.gauss <- function(m, YY, n.tries=10, verbose=FALSE) {
  prev.llk <- -Inf
  model.best <- NULL
  for (n.iter in 1:n.tries) {
    if (verbose) {
      cat('= Fitting model', n.iter, '...\n')
    }
    model <- em.gauss(m, YY, verbose=verbose)
    if (model$llk > prev.llk) {
      model.best <- model
    }
  }
  return(model)
}

set.seed(1)
model <- fit.em.gauss(3, YY, verbose=TRUE)
```

# Model Selection

```{r}
df <- function(model) {
  m <- length(model$mmu)
  d <- length(model$mmu[[1]])
  (m - 1) + m * d + m * d*(d+1) / 2
}

aic.gm <- function(model, YY) {
  2 * (llk(model, YY) - df(model))
}

bic.gm <- function(model, YY) {
  N <- nrow(YY)
  2 * llk(model, YY) - df(model) * log(N)
}
```

Test df, AIC, BIC

```{r}
model <- em.gauss(3, YY, do.plot=FALSE)
df(model)
aic.gm(model, YY)
bic.gm(model, YY)
```

## Select the model ($m$) according to IC

```{r}
ic.mod.sel <- function(m.max, n.tries=10, do.plot=FALSE) {
  aic <- numeric(m.max)
  bic <- numeric(m.max)
  for (m in 1:m.max) {
    cat('m =', m, '\n')
    model <- fit.em.gauss(m, YY, n.tries=n.tries, verbose=FALSE)
    # model <- em.gauss(m, YY)
    aic[m] <- aic.gm(model, YY)
    bic[m] <- bic.gm(model, YY)
  }
  if (do.plot) {
    matplot(cbind(aic, bic), type='b', pch=c(1,2), col=c('blue', 'red'), lty=c('solid'),
            main='', xlab='m')
  }
  legend("topleft", bty = "n",
         legend = c('AIC', 'BIC'),
         col = c('blue', 'red'),
         lty = c('solid', 'solid'))
  return(list(aic=aic, bic=bic))
}
```

```{r}
set.seed(1)
ic.mod.sel(7, 20, do.plot=TRUE)
```

## Cluster data points according to $\mathbf{z}$

```{r}
plot.classification <- function(model, YY, plot.densities=TRUE) {
  N <- nrow(YY)
  m <- length(model$pp)
  ZZ <- matrix(unlist(model$zz), N, m)
  xlim <- c(min(YY[,1]), max(YY[,1]))
  ylim <- c(min(YY[,2]), max(YY[,2]))
  if (plot.densities) {
    plot.mixture(model, YY, main=paste('m =', m))
  } else {
    plot(YY, type='n', main=paste('m =', m))
  }
  class.labels <- apply(ZZ, 1, which.max)
  for (ell in 1:m) {
    par(new=TRUE)
    plot(YY[class.labels==ell,], col=(1+ell), xlab='', ylab='', xlim=xlim, ylim=ylim, axes=FALSE)
  }
}

set.seed(1)
model <- fit.em.gauss(2, YY, 20, verbose=FALSE)
ZZ <- matrix(unlist(model$zz), 272, 2)
head(ZZ)
head(rowSums(ZZ))
# matplot(ZZ)
plot.classification(model, YY)
```

```{r, fig.height=9, fig.width=9}
par(mfrow=c(2,2))
for (ell in 2:5) {
  set.seed(1)
  plot.classification(fit.em.gauss(ell, YY, 30, verbose=FALSE), YY, plot.densities=FALSE)
}
par(mfcol=c(1,1))
```


# Compare with `Mclust`

```{r}
library('mclust')
```

## Classification for $m=2$

```{r, fig.height=9, fig.width=9}
set.seed(1)
par(mfrow=c(2,2))
for (ell in 2:5) {
  set.seed(1)
  M <- Mclust(YY, G=ell, modelNames='VVV')
  plot(M, what='classification')
}
par(mfcol=c(1,1))

par(mfrow=c(2,2))
for (ell in 2:5) {
  set.seed(1)
  M <- Mclust(YY, G=ell, modelNames='VVV')
  plot(M, what='uncertainty')
}
par(mfcol=c(1,1))

par(mfrow=c(2,2))
for (ell in 2:5) {
  set.seed(1)
  M <- Mclust(YY, G=ell, modelNames='VVV')
  plot(M, what='density', type = 'image', col = 'dodgerblue3', grid = 100)
  # plot(M.2.VVV, what='density', type = 'persp')
}
par(mfcol=c(1,1))

```

## df, LLK, BIC for $m=2$

```{r}
set.seed(1)
model.2 <- fit.em.gauss(2, YY, n.tries=20, verbose=FALSE)
plot.mixture(model.2, YY)
df(model.2)
# aic.gm(model, YY)
bic.gm(model.2, YY)
llk(model.2, YY)
```

```{r}
M.2.VVV <- Mclust(YY, G=2, modelNames='VVV')
M.2.VVV$df
M.2.VVV$bic
M.2.VVV$loglik
```

## BIC for several $m$s

```{r}
set.seed(1)
my.bic <- ic.mod.sel(7, 20, do.plot=TRUE)$bic
set.seed(1)
mc.bic <- mclustBIC(YY, modelNames='VVV')
cat('Our BICs:\n')
print(my.bic)
cat('Mclust\'s BICs:\n')
print(as.numeric(mc.bic))
matplot(cbind(mc.bic[1:7], my.bic), pch=c(1,2), type='b')
```

## $\boldsymbol{\Sigma}$ structure according to `Mclust`

```{r}
BIC <- mclustBIC(YY)
plot(BIC)
summary(BIC)
BIC
```