{
    "collab_server" : "",
    "contents" : "---\ntitle: \"4-EM\"\nauthor: \"Dmitry Zotikov\"\ndate: \"April 16, 2017\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n# Plot of the data\n\n```{r}\nlibrary('mvtnorm')\n\nplot.mixture <- function(model, YY, ...) {\n  plot(YY, ...)\n  mmu <- model$mmu\n  SS <- model$SS\n  m <- length(mmu)\n  xx <- seq(min(YY[,1]), max(YY[,1]), by=.1)\n  yy <- seq(min(YY[,2]), max(YY[,2]), by=.1)\n  for (ell in 1:m) {\n    contour(xx, yy, \n             outer(xx, yy, function(xx, yy) dmvnorm(cbind(xx, yy), mmu[[ell]], SS[[ell]])),\n             add=T, col=1+ell%%7)\n  }\n}\n\nYY <- as.matrix(faithful)\npar(mfrow=c(1,2))\n# boxplot(YY$eruptions)\n# boxplot(YY$waiting)\npar(mfrow=c(1,1))\nplot(YY)\n```\n\n# EM & LLK\n\n```{r}\nllk <- function(model, YY) {\n  mmu <- model$mmu\n  SS <- model$SS\n  pp <- model$pp\n  m <- length(mmu)\n  N <- nrow(YY)\n  PPHI <- matrix(0, N, m)\n  for (ell in 1:m) {\n    PPHI[,ell] <- dmvnorm(YY, mean=mmu[[ell]], sigma=SS[[ell]])\n  }\n  sum(log(PPHI %*% pp))\n}\n\nem.gauss <- function(m, YY, llk.diff.eps=0.1, do.plot=FALSE, verbose=FALSE) {\n  d <- ncol(YY)\n  N <- nrow(YY)\n  phi <- dmvnorm\n  mmu <- list()\n  SS <- list()\n  ww <- list()\n  zz <- list()\n  llk.prev <- -Inf\n  llk.hist <- NULL\n  # Initialization\n  # TODO: Split the data into m clusters at random; calculate mmu, SS, pp for each cluster\n  for (ell in 1:m) {\n    # pick two data points at random\n    mmu[[ell]] <- sample(as.data.frame(t(YY)), 1)[[1]]\n    SS[[ell]] <- cov(YY)\n  }\n  pp <- rep(1/m, m)\n  if (do.plot) {\n    plot.mixture(list(pp=pp, mmu=mmu, SS=SS), YY, main=paste('n.iter =', 0))\n  }\n  n.iter <- 1\n  repeat {\n    # Expectation\n    PPHI <- matrix(0, N, m)\n    for (ell in 1:m) {\n      PPHI[,ell] <- phi(YY, mean=mmu[[ell]], sigma=SS[[ell]])\n    }\n    for (ell in 1:m) {\n      zz[[ell]] <- pp[[ell]] * PPHI[,ell]  / PPHI %*% pp\n    }\n    for (ell in 1:m) {\n      ww[[ell]] <- zz[[ell]] / sum(zz[[ell]])\n    }\n    # Maximization\n    for (ell in 1:m) {\n      mmu[[ell]] <- as.vector(t(YY) %*% ww[[ell]])\n      pp[[ell]] <- sum(zz[[ell]]) / N\n      # FIXME: vectorize\n      for (i1 in 1:d) {\n        for (i2 in 1:d) {\n          SS[[ell]][i1, i2] <- sum(ww[[ell]] * (YY[, i1] - mmu[[ell]][i1]) * (YY[, i2] - mmu[[ell]][i2]))\n        }\n      }\n    }\n    llk.cur <- llk(list(pp=pp, mmu=mmu, SS=SS), YY)\n    llk.diff <- abs(llk.cur - llk.prev)\n    if (verbose) {\n      cat('# ', n.iter, ' LLK = ', llk.cur, ' (', llk.diff, ')\\n', sep='')\n    }\n    llk.hist <- c(llk.hist, llk.cur)\n    if (do.plot) {\n      plot.mixture(list(pp=pp, mmu=mmu, SS=SS), YY, main=paste('n.iter =', n.iter))\n    }\n    if (llk.diff < llk.diff.eps) {\n      if (do.plot) {\n        plot(llk.hist, type='o', col='blue', main='Log likelihood', xlab='n.iter')\n      }\n      return(list(pp=pp, mmu=mmu, SS=SS, zz=zz, llk=llk.cur, llk.hist=llk.hist))\n    } else {\n      llk.prev <- llk.cur\n      n.iter <- n.iter + 1\n    }\n  }\n}\n```\n\n## EM for $m = 2$\n\n```{r}\nset.seed(2)\nmodel.2 <- em.gauss(2, YY, do.plot=TRUE, verbose=TRUE)\nmodel.2$pp\nmodel.2$mmu\nmodel.2$SS\n```\n\n## EM for $m = 3$\n\n```{r}\nset.seed(1)\nmodel.3 <- em.gauss(3, YY, do.plot=TRUE, verbose=TRUE)\n```\n\n## Fit for several initial parameters\n\nChoose the best model out of several runs with the highest LLK.\n\n```{r}\nfit.em.gauss <- function(m, YY, n.tries=10, verbose=FALSE) {\n  prev.llk <- -Inf\n  model.best <- NULL\n  for (n.iter in 1:n.tries) {\n    if (verbose) {\n      cat('= Fitting model', n.iter, '...\\n')\n    }\n    model <- em.gauss(m, YY, verbose=verbose)\n    if (model$llk > prev.llk) {\n      model.best <- model\n    }\n  }\n  return(model)\n}\n\nset.seed(1)\nmodel <- fit.em.gauss(3, YY, verbose=TRUE)\n```\n\n# Model Selection\n\n```{r}\ndf <- function(model) {\n  m <- length(model$mmu)\n  d <- length(model$mmu[[1]])\n  (m - 1) + m * d + m * d*(d+1) / 2\n}\n\naic.gm <- function(model, YY) {\n  2 * (llk(model, YY) - df(model))\n}\n\nbic.gm <- function(model, YY) {\n  N <- nrow(YY)\n  2 * llk(model, YY) - df(model) * log(N)\n}\n```\n\nTest df, AIC, BIC\n\n```{r}\nmodel <- em.gauss(3, YY, do.plot=FALSE)\ndf(model)\naic.gm(model, YY)\nbic.gm(model, YY)\n```\n\n## Select the model ($m$) according to IC\n\n```{r}\nic.mod.sel <- function(m.max, n.tries=10, do.plot=FALSE) {\n  aic <- numeric(m.max)\n  bic <- numeric(m.max)\n  for (m in 1:m.max) {\n    cat('m =', m, '\\n')\n    model <- fit.em.gauss(m, YY, n.tries=n.tries, verbose=FALSE)\n    # model <- em.gauss(m, YY)\n    aic[m] <- aic.gm(model, YY)\n    bic[m] <- bic.gm(model, YY)\n  }\n  if (do.plot) {\n    matplot(cbind(aic, bic), type='b', pch=c(1,2), col=c('blue', 'red'), lty=c('solid'),\n            main='', xlab='m')\n  }\n  legend(\"topleft\", bty = \"n\",\n         legend = c('AIC', 'BIC'),\n         col = c('blue', 'red'),\n         lty = c('solid', 'solid'))\n  return(list(aic=aic, bic=bic))\n}\n```\n\n```{r}\nset.seed(1)\nic.mod.sel(7, 20, do.plot=TRUE)\n```\n\n## Cluster data points according to $\\mathbf{z}$\n\n```{r}\nplot.classification <- function(model, YY, plot.densities=TRUE) {\n  N <- nrow(YY)\n  m <- length(model$pp)\n  ZZ <- matrix(unlist(model$zz), N, m)\n  xlim <- c(min(YY[,1]), max(YY[,1]))\n  ylim <- c(min(YY[,2]), max(YY[,2]))\n  if (plot.densities) {\n    plot.mixture(model, YY, main=paste('m =', m))\n  } else {\n    plot(YY, type='n', main=paste('m =', m))\n  }\n  class.labels <- apply(ZZ, 1, which.max)\n  for (ell in 1:m) {\n    par(new=TRUE)\n    plot(YY[class.labels==ell,], col=(1+ell), xlab='', ylab='', xlim=xlim, ylim=ylim, axes=FALSE)\n  }\n}\n\nset.seed(1)\nmodel <- fit.em.gauss(2, YY, 20, verbose=FALSE)\nZZ <- matrix(unlist(model$zz), 272, 2)\nhead(ZZ)\nhead(rowSums(ZZ))\n# matplot(ZZ)\nplot.classification(model, YY)\n```\n\n```{r, fig.height=9, fig.width=9}\npar(mfrow=c(2,2))\nfor (ell in 2:5) {\n  set.seed(1)\n  plot.classification(fit.em.gauss(ell, YY, 30, verbose=FALSE), YY, plot.densities=FALSE)\n}\npar(mfcol=c(1,1))\n```\n\n\n# Compare with `Mclust`\n\n```{r}\nlibrary('mclust')\n```\n\n## Classification for $m=2$\n\n```{r, fig.height=9, fig.width=9}\nset.seed(1)\npar(mfrow=c(2,2))\nfor (ell in 2:5) {\n  set.seed(1)\n  M <- Mclust(YY, G=ell, modelNames='VVV')\n  plot(M, what='classification')\n}\npar(mfcol=c(1,1))\n\npar(mfrow=c(2,2))\nfor (ell in 2:5) {\n  set.seed(1)\n  M <- Mclust(YY, G=ell, modelNames='VVV')\n  plot(M, what='uncertainty')\n}\npar(mfcol=c(1,1))\n\npar(mfrow=c(2,2))\nfor (ell in 2:5) {\n  set.seed(1)\n  M <- Mclust(YY, G=ell, modelNames='VVV')\n  plot(M, what='density', type = 'image', col = 'dodgerblue3', grid = 100)\n  # plot(M.2.VVV, what='density', type = 'persp')\n}\npar(mfcol=c(1,1))\n\n```\n\n## df, LLK, BIC for $m=2$\n\n```{r}\nset.seed(1)\nmodel.2 <- fit.em.gauss(2, YY, n.tries=20, verbose=FALSE)\nplot.mixture(model.2, YY)\ndf(model.2)\n# aic.gm(model, YY)\nbic.gm(model.2, YY)\nllk(model.2, YY)\n```\n\n```{r}\nM.2.VVV <- Mclust(YY, G=2, modelNames='VVV')\nM.2.VVV$df\nM.2.VVV$bic\nM.2.VVV$loglik\n```\n\n## BIC for several $m$s\n\n```{r}\nset.seed(1)\nmy.bic <- ic.mod.sel(7, 20, do.plot=TRUE)$bic\nset.seed(1)\nmc.bic <- mclustBIC(YY, modelNames='VVV')\ncat('Our BICs:\\n')\nprint(my.bic)\ncat('Mclust\\'s BICs:\\n')\nprint(as.numeric(mc.bic))\nmatplot(cbind(mc.bic[1:7], my.bic), pch=c(1,2), type='b')\n```\n\n## $\\boldsymbol{\\Sigma}$ structure according to `Mclust`\n\n```{r}\nBIC <- mclustBIC(YY)\nplot(BIC)\nsummary(BIC)\nBIC\n```",
    "created" : 1492346480641.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "183|6|188|0|\n",
    "hash" : "4207371873",
    "id" : "29B77F3F",
    "lastKnownWriteTime" : 1492625502,
    "last_content_update" : 1492625502250,
    "path" : "~/Documents/СПбГУ/Выч. методы и пакеты в стат. исследованиях/4-EM/2017-03-14.Rmd",
    "project_path" : "2017-03-14.Rmd",
    "properties" : {
        "docOutlineVisible" : "0",
        "last_setup_crc32" : "E5E899DEbb338d19",
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}