---
title: "SVM"
output: html_notebook
---

```{r}
library(e1071)
data(Glass, package='mlbench')
XX <- Glass
XX
plot(XX[-10], col=as.numeric(XX[,10]))
str(XX)
```

```{r}
set.seed(1)
index <- 1:nrow(XX)
testindex <- sample(index, trunc(length(index)/3))
testset <- XX[testindex,]
trainset <- XX[-testindex,]
svm.model <- svm(Type ~ ., data=trainset, cost=100, gamma=1)
svm.model
```

Support vectors

```{r}
pch <- numeric(nrow(XX))
pch[svm.model$index] <- 20
col <- character(nrow(XX))
col[svm.model$index] <- 'red'
col[-svm.model$index] <- 'black'
plot(XX[,-10], pch=pch, col=col)
```

Contingency table

```{r}
svm.pred <- predict(svm.model, testset[,-10])
svm.pred
cont.table <- table(pred=svm.pred, true=testset[,10])
cont.table
classAgreement(cont.table)
```

Overall accuracy

```{r}
set.seed(1)
n.trials <- 100
accuracy.trial.data <- numeric(n.trials)
for (i in 1:n.trials) {
  testindex <- sample(index, trunc(length(index)/3))
  testset <- XX[testindex,]
  trainset <- XX[-testindex,]
  svm.model <- svm(Type ~ ., data=trainset, cost=100, gamma=1)
  svm.pred <- predict(svm.model, testset[,-10])
  cont.table <- table(pred=svm.pred, true=testset[,10])
  accuracy.trial.data[i] <- classAgreement(cont.table)$diag
}
summary(accuracy.trial.data)
```

```{r}
par.svm <- tune.svm(Type~., data=XX, gamma=2^(-1:1), cost=10^(-3:4))
par.svm
plot(par.svm)
```

```{r}
f.C.tune <- function(cc) {
  sapply(cc,
         function(C) tune.svm(Type~., data=XX, gamma=2^(-1:1), cost=C)$best.performance)
}
optimize(f.C.tune, interval=c(0.001, 100))
x.grid <- seq(1, 200, length.out=100)
plot(x.grid, f.C.tune(x.grid), type='o', col='blue')
```

```{r}
f.tune <- function(pp) {
  if (any(pp <= 0)) {
    return(Inf)
  }
  tune.svm(Type~., data=XX, gamma=pp[1], cost=pp[2])$best.performance
}
f.tune(c(1,2))
# nlm(f.tune, c(1, 1), iterlim=10)
# optim(c(1, 10), f.tune, method='SANN', control=list(trace=2, maxit=1000, REPORT=1))
# optim(c(1, 10), f.tune, method='Nelder-Mead', control=list(trace=2, maxit=1000, REPORT=10))
library('nloptr')
set.seed(1)
# nloptr(c(1, 10), f.tune,
#        lb=c(0.01, 0.01),
#        ub=c(10, 100),
#        opts=list('algorithm'='NLOPT_GN_CRS2_LM',
#                  'maxeval'=100,
#                  'print_level'=1,
#                  'xtol_abs'=-1))
# crs2lm(c(1, 10), f.tune, lower=c(0.01, 0.01), upper=c(10, 100), maxeval=100)
nloptr(c(1, 10), 
       f.tune,
       lb=c(0.01, 0.01),
       ub=c(10, 100),
       opts=list('algorithm'='NLOPT_GN_DIRECT',
                 'maxeval'=100,
                 'print_level'=1,
                 'xtol_abs'=-1,
                 'xtol_rel'=-1))
```

# Promoters data set

```{r}
str.to.char <- function(s) {
  unlist(strsplit(s, ''))
}
promoters <- read.csv('Promoters/promoters.data.txt',
                      header = FALSE, as.is = 3, strip.white = TRUE)[,c(1,3)]
str(head(promoters))
bp.to.vec <- function(bp) {
  alphabet <- c('a', 'c', 'g', 't')
  paste(as.character(as.numeric(alphabet == bp)), collapse='')
}
dna.to.bin.vec <- function(dna) {
  as.numeric(str.to.char(paste((sapply(str.to.char(dna), bp.to.vec)), collapse='')))
}
dna.length <- nchar(promoters[1,2])
promoters.bin <- cbind(
  promoters[1],
  as.data.frame(
    matrix(unlist(lapply(promoters[,2], dna.to.bin.vec)), nrow(promoters), dna.length*4, byrow=TRUE)))
names(promoters.bin)[1] <- 'is.promoter'
promoters.bin
# as.numeric(chartr('acgt', '1234', str.to.char(a)))
```

```{r}
set.seed(1)
index <- 1:nrow(promoters.bin)
testindex <- sample(index, trunc(length(index) / 3))
testset <- promoters.bin[testindex,]
trainset <- promoters.bin[-testindex,]
svm.model <- svm(is.promoter ~ ., data=promoters.bin, cost=210.3022, gamma=0.01)
svm.model
svm.pred <- predict(svm.model, testset[,-1])
svm.pred
cont.table <- table(pred=svm.pred, true=testset[,10])
cont.table
classAgreement(cont.table)
```

```{r}
set.seed(1)
train.index <- sample(index, trunc(length(index) / 2))
valid.index <- sample(setdiff(index, train.index), trunc(length(index) / 4))

train.set <- promoters.bin[train.index,]
valid.set <- promoters.bin[valid.index,]
test.set <- promoters.bin[-c(train.index, valid.index),]

test.svm <- function(pp) {
  g <- pp[1]
  C <- pp[2]
  model <- svm(is.promoter ~ ., data=train.set, gamma=g, cost=C)
  prediction <- predict(model, valid.set[,-1])
  sum(valid.set[,1] == prediction) / length(prediction)
}

test.svm(c(1,1))

# f.tune.cross <- function(pp) {
#   if (any(pp <= 0)) {
#     return(Inf)
#   }
#   tune.svm(is.promoter ~ ., data=promoters.bin,
#            tunecontrol=tune.control(sampling='cross', cross=10),
#            gamma=pp[1], cost=pp[2])$best.performance
# }
# f.tune.fix <- function(pp) {
#   if (any(pp <= 0)) {
#     return(Inf)
#   }
#   tune.svm(is.promoter ~ ., data=promoters.bin,
#            tunecontrol=tune.control(sampling='fix', fix=2/3),
#            gamma=pp[1], cost=pp[2])$best.performance
# }
# set.seed(1)
# f.tune.cross(c(1,2))
# set.seed(1)
# f.tune.fix(c(1,2))
library('nloptr')
set.seed(1)
nloptr(c(1, 100),
       test.svm,
       lb=c(0.001, 0.01),
       ub=c(10, 10000),
       opts=list('algorithm'='NLOPT_GN_CRS2_LM',
                 'maxeval'=500,
                 'print_level'=1,
                 'stopval'=0,
                 'xtol_rel'=-1,
                 'xtol_abs'=-1))
```

```{r}
cv <- function(XX, K) {
  train.errs <- numeric(K)
  index <- 1:nrow(XX)
  for (k in 1:K) {
    valid.index <- sample(index, trunc(length(index) / K))
    valid.set <- XX[valid.index,]
    train.set <- XX[-valid.index,]
  }
}
```


```{r}
tune.svm(is.promoter ~ ., data=promoters.bin,
           tunecontrol=tune.control(sampling='fix', fix=2/3),
           gamma=0.01, cost=593.3737)$best.performance
```


